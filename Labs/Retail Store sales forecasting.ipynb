{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38232bit202004salespredictionvenvvenv4cab222feb1243cbaf794862cc2a2344",
   "display_name": "Python 3.8.2 32-bit ('202004SalesPrediction.venv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use case: Sales prediction\n",
    "\n",
    "*Date: 20/04/2020*  \n",
    "*Place: Antwerp, Belgium*  \n",
    "*Data scientist: Anna Sukhareva*  \n",
    "*Contact: anna@linefeed.be*  \n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1 : Business Understanding  \n",
    "2 : Analytic Approach  \n",
    "3 : Data Requirements  \n",
    "4 : Data Collection  \n",
    "5 : Data Understanding  \n",
    "6 : Data Preparation  \n",
    "7 : Modeling  \n",
    "8 : Evaluation  \n",
    "9. Model deployment\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Business Understanding  \n",
    "**Problem:**  There is a network comprising 60 retail shops, selling about 20 K unique items. To manage stock and supply chain , we need to suggest an algorithm for sales prediction for each shop, each item_id.  \n",
    "**Question:** Can we predict sales per item per point of sales?  \n",
    "\n",
    "## 2. Analytic Approach\n",
    "Modeling: machine learning algorithms, such as: Linear regression, Random Forest, XGBoost.   \n",
    "Evaluation: R^2, RMSE.   \n",
    "\n",
    "## 3. Data reguirements:\n",
    "Source: Internal data  \n",
    "Format: csv  \n",
    "Content: information about sales for each item for each shop, during last 2 years.\n",
    "\n",
    "## 4. Data collecion"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# computation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from numpy import inf\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# statistic\n",
    "from pylab import rcParams\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# machine learning\n",
    "from xgboost import XGBRegressor,  plot_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "print('Imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "items = pd.read_csv(r'.\\202004SalesPrediction.data\\items.csv',  dtype={\n",
    "    'item_name': 'str', \n",
    "    'item_id': 'int32', \n",
    "    'item_category_id': 'int32'\n",
    "    }\n",
    "    )\n",
    "item_categories = pd.read_csv(r'.\\202004SalesPrediction.data\\item_categories.csv', dtype={\n",
    "    'item_category_name': 'str', \n",
    "    'item_category_id': 'int32'\n",
    "    }\n",
    "    )\n",
    "shops = pd.read_csv(r'.\\202004SalesPrediction.data\\shops.csv', dtype={\n",
    "    'shop_name': 'str', \n",
    "    'shop_id': 'int32'\n",
    "    }\n",
    "    )\n",
    "test = pd.read_csv(r'.\\202004SalesPrediction.data\\test.csv', dtype={\n",
    "    'ID': 'int32', \n",
    "    'shop_id': 'int32', \n",
    "    'item_id': 'int32'\n",
    "    }\n",
    "    )\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv(r'.\\202004SalesPrediction.data\\sales_train.csv', parse_dates=['date'], dtype={\n",
    "    'date': 'str', \n",
    "    'date_block_num': 'int32', \n",
    "    'shop_id': 'int32', \n",
    "    'item_id': 'int32', \n",
    "    'item_price': 'float32', \n",
    "    'item_cnt_day': 'int32'\n",
    "    }\n",
    "    )\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join datasets\n",
    "\n",
    "train = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5. Data understanding\n",
    "\n",
    "For efficiency, the analysis of data is placed in a separated file, here - only a head of the dataset.   "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naming convension:**\n",
    "\n",
    "Historical features:\n",
    "- date - date of transaction,  \n",
    "- date_block_num - month of transaction, enumerated, starting from the first record in 'train' dataset, the month of the 1st transaction has date_num_block == 1,  \n",
    "- shop_id,  \n",
    "- item_id,  \n",
    "- item_price,  \n",
    "- item_cnt_day - quantity of item in an exact transaction,  \n",
    "\n",
    "Engineered features:  \n",
    "- train_monthly - reduced dataset, having records regarding to \"shop_id\" and \"item_id\" that appear in deployment,  \n",
    "- mean_item_price - mean price item during the months (due to discount policy),  \n",
    "- mean_item_cnt - mean quantity of items sold in 1 month by the shop,  \n",
    "- transaction - number of deals for the item_id during the month,  \n",
    "- item_price_unit - unitary item prices as total sales sum // to quantity, to equalize the discount policy,    \n",
    "- item_cnt_shifted1, item_cnt_shifted2, item_cnt_shifted3 - quantity of sold items in the next/2nd/3rd following month, to reflect the trend,    \n",
    "- item_trend - difference with 'shifted' column, to reflect the trend,  \n",
    "- label - historical quantity of items we're going to use to predict during modeling and to evaluate models performance.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 : Data Preparation  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop dublicates\n",
    "\n",
    "#train.duplicated().sum()\n",
    "train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing outliers  **"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 'item_cnt_day' : removing wholesale deals\n",
    "train.item_cnt_day.plot.box()\n",
    "plt.title('Items quantity per transaction: distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I remove all negative (returns) and > 40:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train.item_cnt_day > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train.item_cnt_day < 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column 'item_price' \n",
    "train.item_price.plot.box()\n",
    "plt.title('Items prices: distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove negative and below 1 and > 60 000"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train.item_price >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train.item_price <= 60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing the datset**  \n",
    "\n",
    "To avoid memory issues on my machine, I reduce train dataset to only \"shop_id\" and \"item_id\" that appear in file 'test' dataset:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shop_ids = test['shop_id'].unique()\n",
    "test_item_ids = test['item_id'].unique()\n",
    "\n",
    "shorted_train = train[train['shop_id'].isin(test_shop_ids)] \n",
    "shorted_train = shorted_train[shorted_train['item_id'].isin(test_item_ids)] \n",
    "\n",
    "# saving changes\n",
    "train_monthly = shorted_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]\n",
    "\n",
    "train_monthly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_monthly.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restructuring the dataset**\n",
    "\n",
    "As we build predictions for month period, we restructure dataset to months time series by groupping data per month and generating monthly statistic:   \n",
    "- 'mean_item_price - mean price item during the months (due to discount policy),   \n",
    "- 'mean_item_cnt' - mean quantity of items sold in 1 month by the shop,  \n",
    "- 'transaction' - number of deals for the item_id during the month  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupping and renaming columns \n",
    "\n",
    "train_monthly = train_monthly.groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False).agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n",
    "train_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions'] \n",
    "train_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and months\n",
    "\n",
    "train_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\n",
    "train_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))\n",
    "\n",
    "train_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating 'label'  **\n",
    "\n",
    "As we predict sales for the next month, as a label we're going to take a number of sold items of next following month. Sample to explain (add a pic):"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a label\n",
    "\n",
    "train_monthly['label'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample - remove after creating a pic\n",
    "train_monthly.loc[(train_monthly.shop_id == 50) & (train_monthly.item_id == 5822) & (train_monthly.date_block_num == 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Features engeneering**\n",
    "\n",
    "We create extra features from data:  \n",
    "- 'item_price_unit' - unitary item prices as total sales sum // to quntity, to equalize еру discount policy,  \n",
    "- 'item_cnt_shifted1', 'item_cnt_shifted2', 'item_cnt_shifted3' - quantity of sold items in the next/2nd/3rd folowing month, to reflect the trend,  \n",
    "- 'item_trend' - difference with 'shifted' column, to reflect the trend"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_price_unit\n",
    "\n",
    "train_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']\n",
    "train_monthly['item_price_unit'].fillna(0, inplace=True)\n",
    "\n",
    "# remove inf\n",
    "train_monthly['item_price_unit'].replace(np.inf, 0, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item_cnt_shifted1, item_cnt_shifted2, item_cnt_shifted3\n",
    "\n",
    "lag_list = [1, 2, 3]\n",
    "\n",
    "for lag in lag_list:\n",
    "    ft_name = ('item_cnt_shifted%s' % lag)\n",
    "    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n",
    "    \n",
    "    # Fill the empty shifted features with 0\n",
    "    train_monthly[ft_name].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item trend\n",
    "\n",
    "train_monthly['item_trend'] = train_monthly['item_cnt']\n",
    "\n",
    "for lag in lag_list:\n",
    "    ft_name = ('item_cnt_shifted%s' % lag)\n",
    "    train_monthly['item_trend'] -= train_monthly[ft_name]\n",
    "\n",
    "train_monthly['item_trend'] /= len(lag_list) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resulting dataset:**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_monthly.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Modeling\n",
    "\n",
    "### Train - validation - split  \n",
    "\n",
    "Our train dataset has 33 date_num_block (equal to 33 months), so we divide data as following:    \n",
    "- train set - date_num_block 3 - 27; we drop first 3 months as they have no data in generated shifted columns,  \n",
    "- validation set - date_num_block 28-32,   \n",
    "\n",
    "** Train and validation sets preparation**"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_monthly.loc[(train_monthly.date_block_num > 2) & (train_monthly.date_block_num < 28)].copy()\n",
    "validation_set = train_monthly.loc[(train_monthly.date_block_num >= 28) & (train_monthly.date_block_num != 33)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate extra features, as mean for items, shops ans item-shop column. We can't use data from test set, that's why we do it after split:  \n",
    "For train set:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFeaturesMeansAndAddToTheTable(table):\n",
    "# Function calculated mean values for shop, item and shop-item in a provided table and writes it down to the table\n",
    "### Sample to use:  createFeaturesMeansAndAddToTheTable(train_set) ###\n",
    "    \n",
    "    df1 = table.groupby(['shop_id']).agg({'label': ['mean']})\n",
    "    df1.columns = ['shop_mean']\n",
    "    df1.reset_index(inplace=True)\n",
    "\n",
    "    # item mean\n",
    "    df2 = table.groupby(['item_id']).agg({'label': ['mean']})\n",
    "    df2.columns = ['item_mean']\n",
    "    df2.reset_index(inplace=True)\n",
    "\n",
    "    # shop-item mean \n",
    "    df3 = table.groupby(['shop_id', 'item_id']).agg({'label': ['mean']})\n",
    "    df3.columns = ['shop_item_mean']\n",
    "    df3.reset_index(inplace=True)\n",
    "\n",
    "    # Add mean features to train set\n",
    "    table = pd.merge(table, df1, on=['shop_id'], how='left')\n",
    "    table = pd.merge(table, df2, on=['item_id'], how='left')\n",
    "    table = pd.merge(table, df3, on=['shop_id', 'item_id'], how='left')\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = createFeaturesMeansAndAddToTheTable(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = createFeaturesMeansAndAddToTheTable(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missed values\n",
    "\n",
    "train_set.dropna(inplace=True)\n",
    "validation_set.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "'Train set:', train_set.shape[0], '\\n',\n",
    "'Validation set:', validation_set.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our resulting train and validation datasets look like:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train and validation sets and labels"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop(['label', 'date_block_num'], axis=1)\n",
    "Y_train = train_set['label'].astype(int)\n",
    "\n",
    "X_validation = validation_set.drop(['label', 'date_block_num'], axis=1)\n",
    "Y_validation = validation_set['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping item categories\n",
    "\n",
    "X_train.drop(['item_category_id'], axis=1, inplace=True)\n",
    "X_validation.drop(['item_category_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling  \n",
    "We build 3 models - linear regression, random forest and XGBoost."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "\n",
    "# Here, we use some of the features\n",
    "lr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\n",
    "\n",
    "lr_train = X_train[lr_features]\n",
    "lr_val = X_validation[lr_features]\n",
    "\n",
    "# Transforming\n",
    "lr_scaler = MinMaxScaler()\n",
    "lr_scaler.fit(lr_train)\n",
    "lr_train = lr_scaler.transform(lr_train)\n",
    "lr_val = lr_scaler.transform(lr_val)\n",
    "\n",
    "# Modeling\n",
    "lr_model = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Fitting\n",
    "lr_model.fit(lr_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "lr_train_pred = lr_model.predict(lr_train)\n",
    "lr_val_pred = lr_model.predict(lr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "\n",
    "# # Here, we use some of the features\n",
    "rf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n",
    "               'item_cnt_shifted1', 'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\n",
    "\n",
    "rf_train = X_train[rf_features]\n",
    "rf_val = X_validation[rf_features]\n",
    "\n",
    "# Modeling\n",
    "rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n",
    "\n",
    "# Fitting\n",
    "rf_model.fit(rf_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "rf_train_pred = rf_model.predict(rf_train)\n",
    "rf_val_pred = rf_model.predict(rf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data\n",
    "\n",
    "# Here, we use some of the features\n",
    "xgb_features = ['item_cnt', 'item_cnt_shifted1', 'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', 'shop_item_mean', 'item_trend', 'mean_item_cnt']\n",
    "\n",
    "xgb_train = X_train[xgb_features]\n",
    "xgb_val = X_validation[xgb_features]\n",
    "\n",
    "# Modeling\n",
    "xgb_model = XGBRegressor(max_depth=8, \n",
    "                         n_estimators=500, \n",
    "                         min_child_weight=1000,  \n",
    "                         colsample_bytree=0.7, \n",
    "                         subsample=0.7, \n",
    "                         eta=0.3, \n",
    "                         seed=0)\n",
    "# Fitting\n",
    "xgb_model.fit(xgb_train, \n",
    "              Y_train, \n",
    "              eval_metric=\"rmse\", \n",
    "              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n",
    "              verbose=20, \n",
    "              early_stopping_rounds=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "xgb_train_pred = xgb_model.predict(xgb_train)\n",
    "xgb_val_pred = xgb_model.predict(xgb_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking features importance\n",
    "plot_importance(xgb_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Evaluation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RMSE = pd.DataFrame(\n",
    "    {\n",
    "        'Model':['Linear Regression', 'Random Forest', 'XGBoost'], \n",
    "        'RMSE':[round(np.sqrt(mean_squared_error(Y_validation, lr_val_pred)), 4), round(np.sqrt(mean_squared_error(Y_validation, rf_val_pred)),4), round(np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)),4)],\n",
    "        'R^2':[round(lr_model.score(lr_val, Y_validation), 2), round(rf_model.score(rf_val, Y_validation), 2), round(xgb_model.score(xgb_val, Y_validation), 2)]\n",
    "    }\n",
    ")\n",
    "df_RMSE = df_RMSE.sort_values('RMSE', ascending=True)\n",
    "df_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "\n",
    "x = range(df_RMSE.shape[0])\n",
    "f = plt.figure(figsize=(16,3))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "R2 = plt.barh(x, df_RMSE['R^2'], color=('darkblue'), zorder=3)\n",
    "plt.xlabel('Explained data, %')\n",
    "plt.yticks(x, df_RMSE['Model'])\n",
    "plt.title('Model performance accuracy R^2, score: the higher - the better')\n",
    "plt.grid(zorder=0)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "RMSE =plt.barh(x, df_RMSE['RMSE'], color=('lightblue'), zorder=3)\n",
    "plt.xlabel('Error of prediction items quantity, +/- pcs')\n",
    "plt.yticks(x, df_RMSE['Model'])\n",
    "plt.title('Model performance error RMSE: the lower- the better')\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a table with right labels and corresponding prediction between different models:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**  \n",
    "Best performance is shown by random forest model with RMSE +/- 2 pcs.  "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model deployment\n",
    "We need to predict sales for some of the shops, for some of the items? for the next month (date_num_bloc = 34). The request to predict look like:"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, ve need to convert data into same structure, as we have in our train and validation sets. "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concating with train and validation sets\n",
    "train_validation_concated = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And merging with our request\n",
    "#deploy_test\n",
    "deploy = pd.merge(test, train_validation_concated, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding permanent values\n",
    "deploy['year'] = 2015\n",
    "deploy['month'] = 9\n",
    "\n",
    "# dropping labels\n",
    "deploy.drop('label', axis=1, inplace=True)\n",
    "\n",
    "# makin same order of columns like train set\n",
    "deploy = deploy[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values\n",
    "sets = [X_train, X_validation, deploy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the median of each shop   \n",
    "for dataset in sets:\n",
    "    for shop_id in dataset['shop_id'].unique():\n",
    "        for column in dataset.columns:\n",
    "            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n",
    "            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with mean's\n",
    "deploy.fillna(deploy.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "rf_deploy = deploy[rf_features]\n",
    "\n",
    "# Predict\n",
    "deploy_prediction = rf_model.predict(rf_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating df\n",
    "\n",
    "deploy_result = test.copy()\n",
    "deploy_result['date_num_block_34_prediction'] = deploy_prediction.round()\n",
    "deploy_result.drop('ID', axis=1, inplace=True)\n",
    "deploy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "End of code."
   ]
  }
 ]
}